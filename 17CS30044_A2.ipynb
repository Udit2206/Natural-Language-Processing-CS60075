{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu\n",
    "\n",
    "Name : Udit Desai      \n",
    "Roll number : 17CS30044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo"
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "file = open('corpus.txt','r+')\n",
    "corpus = file.read()\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize \n",
    "\n",
    "list_of_sentences = sent_tokenize(corpus)\n",
    "\n",
    "for i in range(len(list_of_sentences)):\n",
    "    t = \"\"\n",
    "    s = list_of_sentences[i]\n",
    "    s = s.lower()\n",
    "    for j in range(len(s)):\n",
    "        if(s[j].isalpha()==False):\n",
    "            t += ' '\n",
    "        else:\n",
    "            t += s[j]\n",
    "            \n",
    "    list_of_sentences[i] = t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3oIulBikPua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Unigrams without stopword removal and without smoothing are : \n",
      "the\n",
      "and\n",
      "to\n",
      "a\n",
      "it\n",
      "she\n",
      "i\n",
      "of\n",
      "said\n",
      "you\n",
      "\n",
      "Top 10 Bigrams without stopword removal and without smoothing are : \n",
      "('said', 'the')\n",
      "('of', 'the')\n",
      "('said', 'alice')\n",
      "('in', 'a')\n",
      "('and', 'the')\n",
      "('in', 'the')\n",
      "('it', 'was')\n",
      "('the', 'queen')\n",
      "('to', 'the')\n",
      "('the', 'king')\n",
      "\n",
      "Top 10 Trigrams without stopword removal and without smoothing are : \n",
      "('the', 'mock', 'turtle')\n",
      "('i', 'don', 't')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'hatter')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'mock')\n",
      "('said', 'the', 'caterpillar')\n",
      "('she', 'went', 'on')\n",
      "\n",
      "Top 10 Fourgrams without stopword removal and without smoothing are : \n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('you', 'won', 't', 'you')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('will', 'you', 'won', 't')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('i', 'don', 't', 'know')\n",
      "('as', 'well', 'as', 'she')\n",
      "('well', 'as', 'she', 'could')\n"
     ]
    }
   ],
   "source": [
    "#Write code\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "for content in (list_of_sentences):\n",
    "    words = nltk.word_tokenize(content)\n",
    "    unigrams.extend(words)\n",
    "    bigrams.extend(ngrams(words,2))\n",
    "    trigrams.extend(ngrams(words,3))\n",
    "    fourgrams.extend(ngrams(words,4))\n",
    "    '''if(cnt==1):\n",
    "        print(Counter(bigrams))\n",
    "    cnt+=1\n",
    "    ##similar for trigrams and fourgrams'''\n",
    "'''\n",
    "print(Counter(bigrams).most_common(10))\n",
    "print()'''\n",
    "\n",
    "\n",
    "print(\"Top 10 Unigrams without stopword removal and without smoothing are : \")\n",
    "fdist = nltk.FreqDist(unigrams)\n",
    "for unig in fdist.most_common(10):\n",
    "    print(unig[0])\n",
    "print()\n",
    "print(\"Top 10 Bigrams without stopword removal and without smoothing are : \")\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "for big in fdist.most_common(10):\n",
    "    print(big[0])\n",
    "print()\n",
    "print(\"Top 10 Trigrams without stopword removal and without smoothing are : \")\n",
    "fdist = nltk.FreqDist(trigrams)\n",
    "for trig in fdist.most_common(10):\n",
    "    print(trig[0])\n",
    "print()\n",
    "print(\"Top 10 Fourgrams without stopword removal and without smoothing are : \")\n",
    "fdist = nltk.FreqDist(fourgrams)\n",
    "for fourg in fdist.most_common(10):\n",
    "    print(fourg[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams after stopword removal without smoothing are : \n",
      "said\n",
      "alice\n",
      "little\n",
      "one\n",
      "know\n",
      "like\n",
      "would\n",
      "went\n",
      "could\n",
      "queen\n",
      "\n",
      "Top 10 bigrams after stopword removal without smoothing are : \n",
      "('said', 'the')\n",
      "('said', 'alice')\n",
      "('the', 'queen')\n",
      "('the', 'king')\n",
      "('a', 'little')\n",
      "('mock', 'turtle')\n",
      "('the', 'mock')\n",
      "('the', 'gryphon')\n",
      "('the', 'hatter')\n",
      "('went', 'on')\n",
      "\n",
      "Top 10 trigrams after stopword removal without smoothing are : \n",
      "('the', 'mock', 'turtle')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'hatter')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'mock')\n",
      "('said', 'the', 'caterpillar')\n",
      "('she', 'went', 'on')\n",
      "('she', 'said', 'to')\n",
      "\n",
      "Top 10 fourgrams after stopword removal without smoothing are : \n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('i', 'don', 't', 'know')\n",
      "('as', 'well', 'as', 'she')\n",
      "('well', 'as', 'she', 'could')\n",
      "('in', 'a', 'great', 'hurry')\n",
      "('in', 'a', 'tone', 'of')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "uni_processed = [p for p in unigrams if p not in stopwords]\n",
    "fdist = nltk.FreqDist(uni_processed)\n",
    "print(\"Top 10 unigrams after stopword removal without smoothing are : \")\n",
    "for unig in fdist.most_common(10) :\n",
    "    print(unig[0])\n",
    "print()\n",
    "bi_processed = []\n",
    "for big in bigrams :\n",
    "#    print(big)\n",
    "    if not big[0] in stopwords or not big[1] in stopwords :\n",
    "        bi_processed.append(big)\n",
    "fdist = nltk.FreqDist(bi_processed)\n",
    "print(\"Top 10 bigrams after stopword removal without smoothing are : \")\n",
    "for big in fdist.most_common(10) :\n",
    "    print(big[0])\n",
    "print()\n",
    "tri_processed = []\n",
    "for trig in trigrams :\n",
    "    if not trig[0] in stopwords or not trig[1] in stopwords or not trig[2] in stopwords :\n",
    "        tri_processed.append(trig)\n",
    "fdist = nltk.FreqDist(tri_processed)\n",
    "print(\"Top 10 trigrams after stopword removal without smoothing are : \")\n",
    "for trig in fdist.most_common(10):\n",
    "    print(trig[0])\n",
    "print()\n",
    "\n",
    "four_processed = []\n",
    "for fourg in fourgrams : \n",
    "    if not fourg[0] in stopwords or not fourg[1] in stopwords or not fourg[2] in stopwords or not fourg[3] in stopwords :\n",
    "        four_processed.append(fourg)\n",
    "fdist = nltk.FreqDist(four_processed)\n",
    "print(\"Top 10 fourgrams after stopword removal without smoothing are : \")\n",
    "for fourg in fdist.most_common(10):\n",
    "    print(fourg[0])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams after smoothing are : \n",
      "the\n",
      "and\n",
      "to\n",
      "a\n",
      "it\n",
      "she\n",
      "i\n",
      "of\n",
      "said\n",
      "you\n",
      "\n",
      "\n",
      "Top 10 bigrams after smoothing are : \n",
      "('said', 'the')\n",
      "('of', 'the')\n",
      "('said', 'alice')\n",
      "('in', 'a')\n",
      "('and', 'the')\n",
      "('in', 'the')\n",
      "('it', 'was')\n",
      "('the', 'queen')\n",
      "('to', 'the')\n",
      "('the', 'king')\n",
      "\n",
      "\n",
      "Top 10 trigrams after smoothing are : \n",
      "('the', 'mock', 'turtle')\n",
      "('i', 'don', 't')\n",
      "('the', 'march', 'hare')\n",
      "('said', 'the', 'king')\n",
      "('the', 'white', 'rabbit')\n",
      "('said', 'the', 'hatter')\n",
      "('said', 'to', 'herself')\n",
      "('said', 'the', 'mock')\n",
      "('said', 'the', 'caterpillar')\n",
      "('she', 'went', 'on')\n",
      "\n",
      "\n",
      "Top 10 fourgrams after smoothing are : \n",
      "('said', 'the', 'mock', 'turtle')\n",
      "('she', 'said', 'to', 'herself')\n",
      "('a', 'minute', 'or', 'two')\n",
      "('you', 'won', 't', 'you')\n",
      "('said', 'the', 'march', 'hare')\n",
      "('will', 'you', 'won', 't')\n",
      "('said', 'alice', 'in', 'a')\n",
      "('i', 'don', 't', 'know')\n",
      "('as', 'well', 'as', 'she')\n",
      "('well', 'as', 'she', 'could')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "\n",
    "n = len(unigrams)\n",
    "uniq = set(unigrams)\n",
    "v = len(uniq)\n",
    "fdist = nltk.FreqDist(unigrams)\n",
    "fdist_smooth_u = nltk.FreqDist(unigrams)\n",
    "for uni in uniq :\n",
    "    c = fdist[uni]\n",
    "    fdist_smooth_u[uni] = (c+1)/(n+v)\n",
    "print(\"Top 10 unigrams after smoothing are : \")\n",
    "for uni in fdist_smooth_u.most_common(10) : \n",
    "    print(uni[0])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "n = len(bigrams)\n",
    "uniq = set(bigrams)\n",
    "v = len(uniq)\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "fdist_smooth_b = nltk.FreqDist(bigrams)\n",
    "for big in uniq :\n",
    "    c = fdist[big]\n",
    "    fdist_smooth_b[big] = (c+1)/(n+v)\n",
    "print(\"Top 10 bigrams after smoothing are : \")\n",
    "for big in fdist_smooth_b.most_common(10) : \n",
    "    print(big[0])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "n = len(trigrams)\n",
    "uniq = set(trigrams)\n",
    "v = len(uniq)\n",
    "fdist = nltk.FreqDist(trigrams)\n",
    "fdist_smooth_t = nltk.FreqDist(trigrams)\n",
    "for trig in uniq :\n",
    "    c = fdist[trig]\n",
    "    fdist_smooth_t[trig] = (c+1)/(n+v)\n",
    "print(\"Top 10 trigrams after smoothing are : \")\n",
    "for trig in fdist_smooth_t.most_common(10) : \n",
    "    print(trig[0])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "n = len(fourgrams)\n",
    "uniq = set(fourgrams)\n",
    "v = len(uniq)\n",
    "fdist = nltk.FreqDist(fourgrams)\n",
    "fdist_smooth_f = nltk.FreqDist(fourgrams)\n",
    "for fourg in uniq :\n",
    "    c = fdist[fourg]\n",
    "    fdist_smooth_f[fourg] = (c+1)/(n+v)\n",
    "print(\"Top 10 fourgrams after smoothing are : \")\n",
    "for fourg in fdist_smooth_f.most_common(10) : \n",
    "    print(fourg[0])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for bigram model : \n",
      "For str1 :  \n",
      "\n",
      "['queen', 'king', 'mock', 'gryphon', 'hatter']\n",
      "after that alice said the queen\n",
      "after that alice said the king\n",
      "after that alice said the mock\n",
      "after that alice said the gryphon\n",
      "after that alice said the hatter\n",
      "\n",
      "\n",
      "For str2 :  \n",
      "\n",
      "['a', 'the', 'not', 'going', 'that']\n",
      "alice felt so desperate that she was a\n",
      "alice felt so desperate that she was the\n",
      "alice felt so desperate that she was not\n",
      "alice felt so desperate that she was going\n",
      "alice felt so desperate that she was that\n",
      "\n",
      "\n",
      "Predictions for Trigram model : \n",
      "For str1 :  \n",
      "\n",
      "['king', 'hatter', 'mock', 'caterpillar', 'gryphon']\n",
      "after that alice said the king\n",
      "after that alice said the hatter\n",
      "after that alice said the mock\n",
      "after that alice said the caterpillar\n",
      "after that alice said the gryphon\n",
      "\n",
      "\n",
      "For str2 :  \n",
      "\n",
      "['now', 'quite', 'a', 'walking', 'looking']\n",
      "alice felt so desperate that she was now\n",
      "alice felt so desperate that she was quite\n",
      "alice felt so desperate that she was a\n",
      "alice felt so desperate that she was walking\n",
      "alice felt so desperate that she was looking\n",
      "\n",
      "\n",
      "Predictions for Fourgram model : \n",
      "For str1 :  \n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "For str2 :  \n",
      "\n",
      "['now', 'dozing', 'walking', 'ready', 'in']\n",
      "alice felt so desperate that she was now\n",
      "alice felt so desperate that she was dozing\n",
      "alice felt so desperate that she was walking\n",
      "alice felt so desperate that she was ready\n",
      "alice felt so desperate that she was in\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write code\n",
    "words1 = word_tokenize(str1)\n",
    "words2 = word_tokenize(str2)\n",
    "num = 5\n",
    "pred_b_1 = []\n",
    "pred_b_2 = []\n",
    "pred_t_1 = []\n",
    "pred_t_2 = []\n",
    "pred_f_1 = []\n",
    "pred_f_2 = []\n",
    "cnt = 0\n",
    "for big in fdist_smooth_b.most_common():\n",
    "    if(big[0][0]==words1[-1]):\n",
    "        pred_b_1.append(big[0][1])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for big in fdist_smooth_b.most_common():\n",
    "    if(big[0][0]==words2[-1]):\n",
    "        pred_b_2.append(big[0][1])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for trig in fdist_smooth_t.most_common():\n",
    "    if(trig[0][0]==words1[-2] and trig[0][1]==words1[-1]):\n",
    "        pred_t_1.append(trig[0][2])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for trig in fdist_smooth_t.most_common():\n",
    "    if(trig[0][0]==words2[-2] and trig[0][1]==words2[-1]):\n",
    "        pred_t_2.append(trig[0][2])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for fourg in fdist_smooth_f.most_common():\n",
    "    if(fourg[0][0]==words1[-3] and fourg[0][1]==words1[-2] and fourg[0][2]==words1[-1]):\n",
    "        pred_f_1.append(fourg[0][3])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "cnt = 0\n",
    "for fourg in fdist_smooth_f.most_common():\n",
    "    if(fourg[0][0]==words2[-3] and fourg[0][1]==words2[-2] and fourg[0][2]==words2[-1]):\n",
    "        pred_f_2.append(fourg[0][3])\n",
    "        cnt+=1\n",
    "    if(cnt==num):\n",
    "        break;\n",
    "\n",
    "print(\"Predictions for bigram model : \")\n",
    "print(\"For str1 : \",\"\\n\")\n",
    "print(pred_b_1)\n",
    "for p in pred_b_1:\n",
    "    print(str1,p)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"For str2 : \",\"\\n\")\n",
    "print(pred_b_2)\n",
    "for p in pred_b_2:\n",
    "    print(str2,p)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Predictions for Trigram model : \")\n",
    "print(\"For str1 : \",\"\\n\")\n",
    "print(pred_t_1)\n",
    "for t in pred_t_1:\n",
    "    print(str1,t)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"For str2 : \",\"\\n\")\n",
    "print(pred_t_2)\n",
    "for t in pred_t_2:\n",
    "    print(str2,t)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Predictions for Fourgram model : \")\n",
    "print(\"For str1 : \",\"\\n\")\n",
    "print(pred_f_1)\n",
    "for f in pred_f_1:\n",
    "    print(str1,f)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"For str2 : \",\"\\n\")\n",
    "print(pred_f_2)\n",
    "for f in pred_f_2:\n",
    "    print(str2,f)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
